{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38bf563",
   "metadata": {},
   "source": [
    "### Import all necesaary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd21e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import session\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from urllib.parse import urlparse\n",
    "from requests.exceptions import HTTPError\n",
    "import mysql.connector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6896989b",
   "metadata": {},
   "source": [
    "### Find all API categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e3a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = 'https://rapidapi.com/categories'\n",
    "with session() as s:\n",
    "    r1 = s.get(API_URL)\n",
    "\n",
    "soup = bs(r1.text, 'html.parser')\n",
    "htmlText = soup.prettify()\n",
    "API_Categories= soup.find_all('div',{'CollectionCarousel CollectionCarousel'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872ea69",
   "metadata": {},
   "source": [
    "### Get all API category links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0afaf0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match found.\n"
     ]
    }
   ],
   "source": [
    "# Parse the HTML content\n",
    "soup_all_categories = bs(r1.text, 'html.parser')\n",
    "\n",
    "# Find the <a> tag with the specified class\n",
    "a_tags = soup.find_all('a', class_='CardLink')\n",
    "href_values = []\n",
    "categories = []\n",
    "if a_tags:\n",
    "    for a_tag in a_tags:\n",
    "        href_value = a_tag.get('href')\n",
    "        href_values.append(href_value)\n",
    "        category = a_tag.text.strip()\n",
    "        categories.append(category)      \n",
    "else:\n",
    "    print('No match found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6171f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if href_values:\n",
    "    href_value = [\"https://rapidapi.com\" + href for href in href_values]    \n",
    "    api_href_pairs = list(zip(categories, href_value))\n",
    "    print(api_href_pairs)\n",
    "else:\n",
    "    print('No href value found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657c90c",
   "metadata": {},
   "source": [
    "### Get API links for each category link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')  # Run Chrome in headless mode, without opening a window\n",
    "\n",
    "user_data_dir = r'C:\\Users\\xxx\\AppData\\Local\\Google\\Chrome\\User Data'\n",
    "\n",
    "webdriver_path = 'D:\\chromedriver_win32\\chromedriver.exe'  # Replace with the actual path to the chromedriver executable\n",
    "\n",
    "options.add_argument(f'--user-data-dir={user_data_dir}')\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=webdriver_path, options=options)\n",
    "\n",
    "#for x in href_value:\n",
    "    #print(x)\n",
    "    # Load the initial URL\n",
    "driver.get('https://rapidapi.com/category/News,%20Media')\n",
    "base_urls = []\n",
    "def scrape_page(url):\n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    a_tags = soup.find_all('a', class_='CardLink')\n",
    "    if a_tags:\n",
    "        for a_tag in a_tags:\n",
    "            base_url = \"https://rapidapi.com\" + a_tag.get('href')\n",
    "            base_urls.append(base_url)\n",
    "def scrape_all_pages(base_url):\n",
    "     while True:\n",
    "        scrape_page(driver.current_url)\n",
    "        next_button = None\n",
    "        try:\n",
    "            next_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'r-page-link') and contains(text(),'â€º') and not(@disabled)]\")))\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            WebDriverWait(driver, 10).until(EC.staleness_of(next_button))\n",
    "        except:\n",
    "            if next_button is None:\n",
    "                break\n",
    "\n",
    "scrape_all_pages('https://rapidapi.com/category/News,%20Media')\n",
    "driver.quit()  # Close the browser after scraping    \n",
    "\n",
    "# Print the scraped URLs\n",
    "for url in base_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a504b7",
   "metadata": {},
   "source": [
    "### Get Details from About Page (Followers,Resources URL, Ratings, Votes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "about_details = {}\n",
    "for base_url in base_urls:\n",
    "    try:\n",
    "        resource_url=''\n",
    "        r3 = s.get(base_url+\"details\")\n",
    "        r3.raise_for_status()  # Raise an exception for 4xx or 5xx status codes\n",
    "        soup3 = bs(r3.text, 'html.parser')\n",
    "        sections = soup3.find_all('section')\n",
    "\n",
    "        # Iterate over each section\n",
    "        for section in sections:\n",
    "            followers_div = section.find('div', class_='body1 bold')\n",
    "            if followers_div:\n",
    "                followers_text = followers_div.text.strip()\n",
    "                if followers_text.startswith('Followers:'):\n",
    "                    followers = followers_text[len('Followers:'):].strip()\n",
    "           \n",
    "            resources_div = section.find('div', class_='body1 bold')\n",
    "            #resource_url = ''\n",
    "            if resources_div and 'Resources:' in resources_div.text:\n",
    "                resource_url = section.find('a')['href']\n",
    "        find_ratings= soup3.find('div', class_='ProductRatings')\n",
    "        if find_ratings:\n",
    "            ratings = find_ratings.get_text(strip=True)\n",
    "        else:\n",
    "            ratings = ''\n",
    "        about_details[base_url] = {\n",
    "            'followers': followers,\n",
    "            'resourses': resource_url,\n",
    "            'ratings_votes': ratings\n",
    "        }\n",
    "    except HTTPError as e:\n",
    "        if e.response.status_code == 404:\n",
    "            continue\n",
    "    except NoSuchElementException:\n",
    "        continue \n",
    "print(about_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cc4885",
   "metadata": {},
   "source": [
    "### Upload to Table aboutPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connect to MySQL\n",
    "cnx = mysql.connector.connect(\n",
    "    host='xxx',\n",
    "    user='xxx',\n",
    "    password='xxx',\n",
    "    database='xxx'\n",
    ")\n",
    "\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Prepare the SQL statement for the insert operation\n",
    "insert_about_details = \"INSERT INTO aboutPage (base_url, followers, resources, ratings_votes) VALUES (%s, %s, %s, %s)\"\n",
    "\n",
    "# Iterate over the items in the api_details dictionary\n",
    "for base_url, details in about_details.items():\n",
    "    followers = details['followers']\n",
    "    resourses = details['resourses']\n",
    "    ratings_votes = details['ratings_votes']\n",
    "\n",
    "    # Check if URL already exists in the database\n",
    "    select_query = \"SELECT base_url FROM aboutPage WHERE base_url = %s\"\n",
    "    cursor.execute(select_query, (base_url,))\n",
    "    existing_url = cursor.fetchone()\n",
    "\n",
    "    if existing_url:\n",
    "        print(f\"URL {base_url} already exists in the database. Skipping insertion.\")\n",
    "    else:\n",
    "        # Execute the insert operation for each URL and details\n",
    "        cursor.execute(insert_about_details, (base_url, followers, resourses, ratings_votes))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "cnx.commit()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13266fa",
   "metadata": {},
   "source": [
    "### Get Documentation word count length for each API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fafe927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MySQL\n",
    "cnx = mysql.connector.connect(\n",
    "    host='xxx',\n",
    "    user='xxx',\n",
    "    password='xxx',\n",
    "    database='xxx'\n",
    ")\n",
    "\n",
    "cursor = cnx.cursor()\n",
    "select_query = \"SELECT base_url FROM aboutPage\"\n",
    "\n",
    "cursor.execute(select_query)\n",
    "\n",
    "# Fetch all the results\n",
    "results = cursor.fetchall()\n",
    "\n",
    "# Extract base_urls from results\n",
    "base_urls = [row[0] for row in results]\n",
    "\n",
    "# Close cursor and connection\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f1dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url_div = {}\n",
    "\n",
    "for base_url in base_urls:\n",
    "    r4 = s.get(base_url + \"details\")\n",
    "\n",
    "\n",
    "    soup4 = bs(r4.text, 'html.parser')\n",
    "    Documentation = []\n",
    "    Documentation = soup4.find_all(\"div\", class_=\"MarkdownPreview\")\n",
    "    word_count = 0\n",
    "\n",
    "    for div in Documentation:\n",
    "        text = div.get_text()  # Get the text content within the div\n",
    "        words = text.split()  # Split the text into words\n",
    "        word_count = len(words)  # Count the number of words\n",
    "    base_url_div[base_url] = word_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acdf4d",
   "metadata": {},
   "source": [
    "### Update aboutPage table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd09d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing a connection to the MySQL database\n",
    "cnx = mysql.connector.connect(\n",
    "    host='xxx',\n",
    "    user='xxx',\n",
    "    password='xxx',\n",
    "    database='xxx'\n",
    ")\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Iterating through base_url_div dictionary and updating database\n",
    "for base_url, div_value in base_url_div.items():\n",
    "    update_query = f\"UPDATE aboutPage SET documentation = '{div_value}' WHERE base_url = '{base_url}'\"\n",
    "    cursor.execute(update_query)\n",
    "\n",
    "# Committing changes and closing cursor and connection\n",
    "cnx.commit()\n",
    "cursor.close()\n",
    "cnx.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
