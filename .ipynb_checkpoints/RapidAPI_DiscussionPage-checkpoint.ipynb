{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38bf563",
   "metadata": {},
   "source": [
    "### Import all necesaary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd21e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import session\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from urllib.parse import urlparse\n",
    "from requests.exceptions import HTTPError\n",
    "import mysql.connector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6896989b",
   "metadata": {},
   "source": [
    "### Find all API categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e3a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = 'https://rapidapi.com/categories'\n",
    "with session() as s:\n",
    "    r1 = s.get(API_URL)\n",
    "\n",
    "soup = bs(r1.text, 'html.parser')\n",
    "htmlText = soup.prettify()\n",
    "API_Categories= soup.find_all('div',{'CollectionCarousel CollectionCarousel'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872ea69",
   "metadata": {},
   "source": [
    "### Get all API category links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0afaf0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match found.\n"
     ]
    }
   ],
   "source": [
    "# Parse the HTML content\n",
    "soup_all_categories = bs(r1.text, 'html.parser')\n",
    "\n",
    "# Find the <a> tag with the specified class\n",
    "a_tags = soup.find_all('a', class_='CardLink')\n",
    "href_values = []\n",
    "categories = []\n",
    "if a_tags:\n",
    "    for a_tag in a_tags:\n",
    "        href_value = a_tag.get('href')\n",
    "        href_values.append(href_value)\n",
    "        category = a_tag.text.strip()\n",
    "        categories.append(category)      \n",
    "else:\n",
    "    print('No match found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6171f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if href_values:\n",
    "    href_value = [\"https://rapidapi.com\" + href for href in href_values]    \n",
    "    api_href_pairs = list(zip(categories, href_value))\n",
    "    print(api_href_pairs)\n",
    "else:\n",
    "    print('No href value found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657c90c",
   "metadata": {},
   "source": [
    "### Get API links for each category link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')  # Run Chrome in headless mode, without opening a window\n",
    "\n",
    "user_data_dir = r'C:\\Users\\xxx\\AppData\\Local\\Google\\Chrome\\User Data'\n",
    "\n",
    "webdriver_path = 'D:\\chromedriver_win32\\chromedriver.exe'  # Replace with the actual path to the chromedriver executable\n",
    "\n",
    "options.add_argument(f'--user-data-dir={user_data_dir}')\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=webdriver_path, options=options)\n",
    "\n",
    "#for x in href_value:\n",
    "    #print(x)\n",
    "    # Load the initial URL\n",
    "driver.get('https://rapidapi.com/category/News,%20Media')\n",
    "base_urls = []\n",
    "def scrape_page(url):\n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    a_tags = soup.find_all('a', class_='CardLink')\n",
    "    if a_tags:\n",
    "        for a_tag in a_tags:\n",
    "            base_url = \"https://rapidapi.com\" + a_tag.get('href')\n",
    "            base_urls.append(base_url)\n",
    "def scrape_all_pages(base_url):\n",
    "     while True:\n",
    "        scrape_page(driver.current_url)\n",
    "        next_button = None\n",
    "        try:\n",
    "            next_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'r-page-link') and contains(text(),'›') and not(@disabled)]\")))\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            WebDriverWait(driver, 10).until(EC.staleness_of(next_button))\n",
    "        except:\n",
    "            if next_button is None:\n",
    "                break\n",
    "\n",
    "scrape_all_pages('https://rapidapi.com/category/News,%20Media')\n",
    "driver.quit()  # Close the browser after scraping    \n",
    "\n",
    "# Print the scraped URLs\n",
    "for url in base_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a504b7",
   "metadata": {},
   "source": [
    "### Get Details from Discussion Page \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "user_data_dir = r'C:\\Users\\xxx\\AppData\\Local\\Google\\Chrome\\User Data'\n",
    "webdriver_path = 'D:\\chromedriver_win32\\chromedriver.exe'\n",
    "options.add_argument(f'--user-data-dir={user_data_dir}')\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=webdriver_path, options=options)\n",
    "discussion_details = {}\n",
    "for base_url in base_urls:\n",
    "    driver.get(base_url+\"discussions\")\n",
    "    wait = WebDriverWait(driver, 5)\n",
    "    try:\n",
    "        next_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'r-page-link') and contains(text(),'»') and not(@disabled)]\")))\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "    \n",
    "        li_element = WebDriverWait(driver, 5).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"li.r-page-item.selected\")))\n",
    "    \n",
    "        soup = bs(driver.page_source, 'html.parser')\n",
    "    \n",
    "        if li_element:\n",
    "            soup_li_element = bs(li_element.get_attribute('outerHTML'), 'html.parser')\n",
    "            button_element = soup_li_element.find(\"button\", class_=\"r-page-link\")\n",
    "        \n",
    "            if button_element:\n",
    "                discussion_page_count = button_element.get_text(strip=True)\n",
    "                print(\"Value:\", discussion_page_count)\n",
    "            else:\n",
    "                print(\"Button element not found.\")\n",
    "        else:\n",
    "            print(\"LI element not found.\")\n",
    "        \n",
    "    except TimeoutException:\n",
    "        try:\n",
    "            div_element = driver.find_element_by_class_name(\"REmptyState\")\n",
    "            discussion_page_count = 0\n",
    "        except NoSuchElementException :\n",
    "            discussion_page_count = 1\n",
    "    discussion_details[base_url] = {\n",
    "        'page_count': discussion_page_count\n",
    "    }\n",
    "driver.quit()\n",
    "print(discussion_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cc4885",
   "metadata": {},
   "source": [
    "### Upload to Table discussionPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Connect to MySQL\n",
    "cnx = mysql.connector.connect(\n",
    "    host='xxx',\n",
    "    user='xxx',\n",
    "    password='xxx',\n",
    "    database='xxx'\n",
    ")\n",
    "\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Prepare the SQL statement for the insert operation\n",
    "insert_discussion_details = \"INSERT INTO discussionPage (base_url, pageCount) VALUES (%s, %s)\"\n",
    "\n",
    "# Iterate over the items in the api_details dictionary\n",
    "for base_url, details in discussion_details.items():\n",
    "    page_count = details['page_count']\n",
    "\n",
    "    # Check if URL already exists in the database\n",
    "    select_query = \"SELECT base_url FROM discussionPage WHERE base_url = %s\"\n",
    "    cursor.execute(select_query, (base_url,))\n",
    "    existing_url = cursor.fetchone()\n",
    "\n",
    "    if existing_url:\n",
    "        print(f\"URL {base_url} already exists in the database. Skipping insertion.\")\n",
    "    else:\n",
    "        # Execute the insert operation for each URL and details\n",
    "        cursor.execute(insert_discussion_details, (base_url, page_count))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "cnx.commit()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45443f",
   "metadata": {},
   "source": [
    "### Get APIs with discussion count page atleast 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MySQL\n",
    "cnx = mysql.connector.connect(\n",
    "    host='xxx',\n",
    "    user='xxx',\n",
    "    password='xxx',\n",
    "    database='xxx'\n",
    ")\n",
    "\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Execute a SQL query to select all URLs from the discussionPage table\n",
    "# cursor.execute(\"SELECT base_url FROM discussionPage\")\n",
    "cursor.execute(\"select base_url from discussionPage where pageCount=1\")\n",
    "\n",
    "# Fetch all the rows (URLs) from the result set\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Extract the URLs from the rows\n",
    "urls = [row[0] for row in rows]\n",
    "print(len(urls))\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "cnx.close()\n",
    "\n",
    "# Print the fetched URLs\n",
    "for url in urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4686e6d8",
   "metadata": {},
   "source": [
    "### Update discussions having replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bebf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize a variable to store the total count of comments\n",
    "total_comments = 0\n",
    "cnx = mysql.connector.connect(\n",
    "    host='xxx',\n",
    "    user='xxx',\n",
    "    password='xxx',\n",
    "    database='xxx'\n",
    ")\n",
    "\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Batch size for updates\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize a counter for the number of updates\n",
    "update_count = 0\n",
    "\n",
    "# Iterate over each base_url\n",
    "for base_url in urls:\n",
    "    # Construct the discussions URL based on the base URL\n",
    "    discussions_url = base_url + \"discussions\"\n",
    "\n",
    "    # Send a GET request to the discussions URL\n",
    "    response = requests.get(discussions_url)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all divs with class \"IssueComments\" within divs with class \"IssueCard\"\n",
    "    issue_comments_divs = soup.select(\".IssuessList .IssueCard .IssueComments\")\n",
    "\n",
    "    # Initialize a variable to store the count of comments for the current URL\n",
    "    url_comments = 0\n",
    "\n",
    "    # Iterate over each div and extract the count of comments\n",
    "    for div in issue_comments_divs:\n",
    "        comments_text = div.get_text(strip=True)\n",
    "        count_of_comments = int(comments_text.split()[0])  # Extract the count of comments as an integer\n",
    "        url_comments += count_of_comments\n",
    "\n",
    "    # Update the database with the count of comments for the current URL\n",
    "    print(f\"Total count of comments for {base_url} {url_comments}\")\n",
    "    cursor.execute(f\"UPDATE discussionPage SET pageComment = {url_comments} WHERE base_url = '{base_url}'\")\n",
    "\n",
    "    # Increment the update count\n",
    "    update_count += 1\n",
    "\n",
    "    # If the batch size is reached, commit the changes and reset the update count\n",
    "    if update_count % batch_size == 0:\n",
    "        cnx.commit()\n",
    "        print(\"Batch committed\")\n",
    "\n",
    "# Commit any remaining changes and close the cursor and connection\n",
    "cnx.commit()\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
